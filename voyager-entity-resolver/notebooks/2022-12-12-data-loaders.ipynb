{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors Lookup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Initial exploration of using word vectors and FAISS indexes\n",
    "\n",
    "```\n",
    "{'power': 'generator', 'generator:type': 'solar_photovoltaic_panel', 'generator:method': 'photovoltaic', 'generator:source': 'solar', 'generator:output:electricity': 'yes'}\n",
    "```\n",
    "becomes\n",
    "```\n",
    "power generator generator type solar photovoltaic panel generator method photovoltaic generator source solar generator output electricity \n",
    "```\n",
    "\n",
    "Each token in the document, like \"power\" or \"generator\", is assigned is mapped to a word vector. The vector of the document is the mean of the individual word embedding vectors.\n",
    "\n",
    "Spacy was used for preprocessing, with FastText word embeddings.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "No problems using FAISS, but did not try to optimize speed of retrieval.\n",
    "\n",
    "Word embeddings without training don't look particularly effective (opinion).\n",
    "\n",
    "Ways the semantic search can be improved.\n",
    "\n",
    "Needs better preprocessing of documents\n",
    "\n",
    "- remove names\n",
    "- remove addresses\n",
    "- remove contact info\n",
    "- remove \"amenity\" word\n",
    "\n",
    "Word embeddings have inherent problems because they are context free. See the coffee shop example.\n",
    "These problems could be minimized by training the word vectors using the OSM tags. However, that would\n",
    "move the meaning of words further from their usage in natural language. If all tags were treated as \n",
    "different tokens, e.g. `key:amenity`, `value:shop`, `key:cuisine`, `value:coffee_shop` - then training\n",
    "with documents like \"coffee shop key:amenity value:cafe key:cuisine value:coffee_shop\" would provide \n",
    "a bridge between natural language and the keys.\n",
    "\n",
    "However, moving to sentence embeddings is likely a better option to handle context.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\"postgresql+psycopg2://jeffreyarnold@localhost:5432/osm\", future=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "with engine.connect() as conn:\n",
    "    features = conn.execute(text(\"SELECT osm_id, tags FROM  osm TABLESAMPLE SYSTEM (1) LIMIT 10\")).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import spacy\n",
    "from typing import List\n",
    "import itertools\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "class Documentizer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def _key_cleaner(self, key: str) -> str:\n",
    "        key = re.sub(\"[_:]\", \" \", key)\n",
    "        return key\n",
    "    \n",
    "    def _value_cleaner(self, value: str) -> str:\n",
    "        # remove yes/no values\n",
    "        value = value.replace(\"yes\", \"\").replace(\"no\", \"\").strip()\n",
    "        value = re.sub(\"[_]\", \" \", value)\n",
    "        return value  \n",
    "    \n",
    "    def _tag_filter(self, key: str, value: str) -> List[str]:\n",
    "        # If NO then remove the word\n",
    "        if value == \"no\":\n",
    "            return []\n",
    "        return [self._key_cleaner(key), self._value_cleaner(value)]\n",
    "    \n",
    "    def __call__(self, tags: Dict[str, Any]) -> str:\n",
    "        return \" \".join(itertools.chain.from_iterable(self._tag_filter(k, v) for k, v in tags.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    res = conn.execute(text(\"SELECT tags from osm WHERE length(tags::TEXT) > 100 LIMIT 1\")).fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG:  {'power': 'generator', 'generator:type': 'solar_photovoltaic_panel', 'generator:method': 'photovoltaic', 'generator:source': 'solar', 'generator:output:electricity': 'yes'}\n",
      "Document:  power generator generator type solar photovoltaic panel generator method photovoltaic generator source solar generator output electricity \n"
     ]
    }
   ],
   "source": [
    "print(\"TAG: \", res['tags'])\n",
    "print(\"Document: \", Documentizer()(res['tags']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2 as j2\n",
    "ENV = j2.Environment(undefined=j2.StrictUndefined)\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "if not Doc.has_extension(\"osm_id\"):\n",
    "    Doc.set_extension(\"osm_id\", default=None)\n",
    "    \n",
    "# Oakland bbox\n",
    "bbox = (-122.350244,37.698079,-122.112321,37.850861)\n",
    "\n",
    "query =\"\"\"\n",
    "SELECT osm_id, tags\n",
    "FROM osm\n",
    "WHERE geom && ST_MakeEnvelope({{bbox | join(\",\")}}, 4326)\n",
    "    AND osm_type = 'N'\n",
    "LIMIT 500000\n",
    "\"\"\"\n",
    "\n",
    "query = ENV.from_string(query).render(bbox=bbox)\n",
    "\n",
    "docs = []\n",
    "\n",
    "import itertools\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    features = conn.execute(text(query))\n",
    "    documents = ((nlp(Documentizer()(row.tags)), {\"osm_id\": row.osm_id, \"tags\": row.tags}) for row in features)\n",
    "    for doc, ctx in nlp.pipe(documents, as_tuples=True):\n",
    "        vec = doc.vector / doc.vector_norm if doc.vector_norm > 0 else doc.vector\n",
    "        docs.append((ctx, vec))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 39 thousand values in that bbox\n",
    "\n",
    "```\n",
    "SELECT count(*)\n",
    " FROM osm\n",
    " WHERE geom && ST_MakeEnvelope(-122.350244,37.698079,-122.112321,37.850861, 4326)\n",
    "     AND osm_type = 'N'\n",
    " \n",
    "+-------+\n",
    "| count |\n",
    "|-------|\n",
    "| 39431 |\n",
    "+-------+\n",
    "SELECT 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39431, 300)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as npb\n",
    "embeddings = np.vstack([d[1] for d in docs])\n",
    "embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word embeddings into a FAISS index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the choices of the FAISS index are optimized\n",
    "\n",
    "- Using inner product (IP) and a unit vector is equivalent to cosine similarity\n",
    "- Using Flat index to see how the lookup works. The other indexes are for computational reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "embedding_size = embeddings.shape[1]\n",
    "\n",
    "#Defining our FAISS index\n",
    "#Number of clusters used for faiss. Select a value 4*sqrt(N) to 16*sqrt(N) - https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n",
    "# n_clusters = int(2 * np.sqrt(embeddings.shape[0]))\n",
    "\n",
    "#We use Inner Product (dot-product) as Index. We will normalize our vectors to unit length, then is Inner Product equal to cosine similarity\n",
    "# quantizer = faiss.IndexFlatIP(embedding_size)\n",
    "# index = faiss.IndexIVFFlat(quantizer, embedding_size, n_clusters, faiss.METRIC_INNER_PRODUCT)\n",
    "index = faiss.IndexFlatIP(embedding_size)  \n",
    "\n",
    "# Then we train the index to find a suitable clustering\n",
    "index.train(embeddings)\n",
    "\n",
    "# Finally we add all embeddings to the index\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.9999999, 0.9999999, 0.9999999, 0.9999999, 0.9999999],\n",
       "        [0.9999999, 0.9999999, 0.9999999, 0.9999999, 0.9999999],\n",
       "        [0.9999999, 0.9999999, 0.9999999, 0.9999999, 0.9999999],\n",
       "        [0.9999999, 0.9999999, 0.9999999, 0.9999999, 0.9999999],\n",
       "        [0.9999999, 0.9999999, 0.9999999, 0.9999999, 0.9999999]],\n",
       "       dtype=float32),\n",
       " array([[60, 40,  2,  1,  0],\n",
       "        [60, 40,  2,  1,  0],\n",
       "        [60, 40,  2,  1,  0],\n",
       "        [48,  8,  7,  4,  3],\n",
       "        [48,  8,  7,  4,  3]]))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search(embeddings[:5, :], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amenity cafe',\n",
       " 'amenity cafe',\n",
       " 'amenity cafe cuisine coffee shop',\n",
       " 'name Paddington Cafe amenity cafe cuisine coffee shop',\n",
       " 'name Aroma Cafe amenity cafe cuisine coffee shop']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"cafe\"\n",
    "qv = nlp(query).vector \n",
    "qv /= np.linalg.norm(qv)\n",
    "dist, idx = index.search(qv.reshape(1, -1), 5)\n",
    "[Documentizer()(doc_values[i]['tags']) for i in idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amenity cafe cuisine coffee shop',\n",
       " 'shop laundry',\n",
       " 'shop laundry',\n",
       " 'shop hairdresser',\n",
       " 'shop convenience']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"coffee shop\"\n",
    "qv = nlp(query).vector \n",
    "qv /= np.linalg.norm(qv)\n",
    "dist, idx = index.search(qv.reshape(1, -1), 5)\n",
    "[Documentizer()(doc_values[i]['tags']) for i in idx[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see problems with word vectors and the synthetic documents being used. \n",
    "\n",
    "- \"coffee\" and \"shop\" are given equal prominence\n",
    "- So any entry with \"shop\" that is short - returns a good value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name Uarhi Taqueria amenity fast food cuisine mexican',\n",
       " 'name Pieology amenity fast food cuisine pizza',\n",
       " 'name Guadalajara Food Truck amenity fast food cuisine mexican',\n",
       " 'name Sushi GoGo amenity fast food cuisine sushi delivery ',\n",
       " 'name Dimond Kitchen amenity fast food cuisine mexican']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"ethiopian food\"\n",
    "qv = nlp(query).vector \n",
    "qv /= np.linalg.norm(qv)\n",
    "dist, idx = index.search(qv.reshape(1, -1), 5)\n",
    "[Documentizer()(doc_values[i]['tags'])  for i in idx[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picks up \"food\" but does not get ethiopian values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name Lemat phone +15104302717 amenity restaurant cuisine ethiopian website https://www.lematethiorestaurant.com/ addr street Adeline Street addr postcode 94703 opening hours We-Mo 12:00-22:00 addr housenumber 3212\n",
      "name Shewhat cafe phone 5102509533 amenity restaurant cuisine ethiopian\n",
      "name Cafe Colucci amenity restaurant cuisine ethiopian addr city Berkeley addr state CA addr street Telegraph Avenue addr housenumber 6427\n",
      "name Asmara phone (510) 547-5100 amenity restaurant cuisine ethiopian website http://asmararestaurant.com/ takeaway  addr city Oakland wheelchair  addr street Telegraph Avenue addr postcode 94609 opening hours Tu-Su 11:30-22:30\n",
      "name Enssaro amenity restaurant cuisine ethiopian website http://www.enssaro.com addr city Oakland addr street Grand Avenue addr postcode 94610 opening hours Mo, We, Th, Su 11:30-22:00; Fr, Sa 11:30-23:00 addr housenumber 357\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    res = conn.execute(text(ENV.from_string(\"\"\"\n",
    "        SELECT tags from osm \n",
    "        WHERE tags ->> 'cuisine' ILIKE '%ethiopian%' \n",
    "        AND geom && ST_MakeEnvelope({{bbox | join(\",\")}}, 4326)\n",
    "        AND osm_type = 'N'\n",
    "        LIMIT 5\"\"\").render(bbox=bbox))).fetchall()\n",
    "for row in res:\n",
    "    print(Documentizer()(row.tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name Jackson Hewitt office tax advisor addr housenumber 2137',\n",
       " 'name Dentist Bryan D. Haynes amenity dentist healthcare dentist',\n",
       " 'office tax advisor',\n",
       " 'name Alameda County Sheriff amenity police',\n",
       " 'name Dr. Huey P. Newton Foundation office ngo website https://hueypnewtonfoundation.org/']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"attorney\"\n",
    "qv = nlp(query).vector \n",
    "qv /= np.linalg.norm(qv)\n",
    "dist, idx = index.search(qv.reshape(1, -1), 5)\n",
    "[Documentizer()(doc_values[i]['tags']) for i in idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name Neptune Palace Hotel tourism hotel',\n",
       " 'name Pullman Hotel demolished tourism hotel',\n",
       " 'name Arcadia Hotel demolished tourism hotel',\n",
       " 'amenity restaurant',\n",
       " 'amenity restaurant',\n",
       " 'amenity restaurant',\n",
       " 'amenity restaurant',\n",
       " 'name Hotel Pine demolished tourism hotel',\n",
       " 'name Trending Inn tourism hotel',\n",
       " 'amenity cafe']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"hotel\"\n",
    "qv = nlp(query).vector \n",
    "qv /= np.linalg.norm(qv)\n",
    "dist, idx = index.search(qv.reshape(1, -1), 10)\n",
    "[Documentizer()(doc_values[i]['tags']) for i in idx[0] if i >= 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hotel is picking up some hotels and models - but also restraunts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4212373f8085a675b26b7c86407d61877f46627c779e666d726e7ed1440a9e8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
